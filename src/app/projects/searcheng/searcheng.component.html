<div class="text-container read">
    <article class="read-wrap">
        <h1 style="animation: transitionIn 0.75s; font-size: 58px; display: flex; justify-content: center;">
            <div>Search Engine</div>
        </h1>
        <div class="img-container" style="animation: transitionIn 1.5s; margin-top: 20px;">
            <img height="600" width="1200" src="https://hahafhahalpaca-1305808520.cos.ap-shanghai.myqcloud.com/search_demo.png">
        </div>
        <div class="p-content" data-aos="zoom-in-up">
            <h3>
                A search engine I created while working for MiraVideo, where you could search videos/images by tags, video meta (height, width, duration, fps, etc.). 
                Or you could even search videos/images with another video/image (based on embeddings). Currently there are 3 types of features supported, 
                including person feature, place feature, and action feature. 
                <br>
                <br> This search engine essentially consists of 4 parts. 
                <br>
                <br>1. Data extraction from videos. Tags are extracted with the video_tag_tsn_lstm model release by PaddlePaddle. Video meta information is
                extracted using FFMPEG. Each video will have its own ttt.json file once all extractions are complete. Now all the information stored in the ttt.json files
                will be indexed to Elastic Search.
            </h3>
        </div>
    </article>
</div>

<div class="img-container" data-aos="zoom-in-down">
    <iframe
        src="https://paddlepaddle.github.io/Paddle-Lite/v2.0.0-rc/source_compile/"
        style="width: 1350px; height:600px; background-color: white;"></iframe>
</div>

<div class="text-container read" style="background-color: #A2978F; margin-top: 30px;" data-aos="zoom-in-up">
    <article class="read-wrap">
        <h3 class="p-content" data-aos="zoom-in-up">
            2. Video/frame embeddings are extracted with MovieNet. These embeddings include person, place, action features. 
            Now we need to insert these extracted embeddings to Milvus. I had to create a ES_id to Milvus_id map since our vector database does not support
            indexing strings. I need to save a hash id while inserting some embeddings into milvus. Then create a ES_id (generated when inserting the video to ES) to Milvus_id mapping.
        </h3>
    </article>
</div>

<div class="img-container" data-aos="zoom-in-down" style="background-color: #A2978F;">
    <iframe
        src="https://milvus.io/"
        style="width: 1350px; height:600px; background-color: #A2978F;"></iframe>
</div>

<div class="text-container read" style=" margin-top: 30px;" data-aos="zoom-in-up">
    <article class="read-wrap">
        <h3 class="p-content" data-aos="zoom-in-up">
            3. Create the search request APIs. All the backend APIs are written in Python3 with Flask. Since loading the models and inferencing requires GPUs, I decided to use 
            Redis Queue workers for doing the actual extraction when an request is recieved. 
            <br>
            <br>
            The actual procedure goes like this. First, users use the website and send a search
            request from their browser. The API server will recieve this request, construct a search query, and then pass this query along with other parameters to a GPU machine 
            with a worker listening to a specific Redis Queue. Once this Redis Queue recieves the job call from the API server, the GPU machine will start extracting 
            different data/features. After the extraction completes, the GPU machine will perform the actual searching based on the query passed in as well as the data/features just extracted,
            look for similar videos in ES or Milvus depends on what the users are searching for. After the search completes, the API server will read the result of this RQ job, jsonify it, 
            then send this reponse back to the browser. 
        </h3>
    </article>
</div>

<div class="img-container" data-aos="zoom-in-down">
    <iframe src="https://mmlab.ie.cuhk.edu.hk/projects.html" style="width: 1350px; height:600px;"></iframe>
</div>

<div class="text-container read" style=" margin-top: 30px; background-color: #A2978F;" data-aos="zoom-in-up">
    <article class="read-wrap">
        <h3 class="p-content" data-aos="zoom-in-up">
            4. Front end is written with React. Upon the completion of step 3, our front end will finally recieve the search result reponse, then map all the information in this respond to
            an asset state hook. Each asset will contain a video URL, video meta information, video embedding information. After all the assets are successfully created, the result
            will be displayed to the users' browser, shown in the picture attached at the very top of this page. You can try it with the MiraVideo video databse here. (If it is still working
            , I think someone is still maintaining it? not sure) 
            click the black square blocks on the left for two times, then click the black square block in the middle, you will see a little blue folder icon, then click that button
            , you will enter the search page (yeah I know, the search engine was embedded in one of our video editors) 
            You could upload some image/videos and search for similar ones, or just simply choose/type in a tag and search for that. 
        </h3>
    </article>
</div>

<script>
    AOS.init()
</script>
